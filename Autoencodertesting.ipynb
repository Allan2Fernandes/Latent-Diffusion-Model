{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from DatasetBuilder import DatasetBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import DDPMScheduler, StableDiffusionPipeline\n",
    "import numpy\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "diffusion_model_epoch = 46\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "directory = \"C:/Users/allan/Downloads/FacesDatasetDepth\"\n",
    "img_size = 512\n",
    "batch_size = 4\n",
    "device = torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = torch.load(f\"Models/Epoch{diffusion_model_epoch}.pt\").to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32c3c56470534018a310711205229a74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allan\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "vae = StableDiffusionPipeline.from_pretrained(model_id).to(torch.device(device)).vae\n",
    "vae.enable_xformers_memory_efficient_attention()\n",
    "vae_scaling_factor = vae.config.scaling_factor\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='linear')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def decode_latents_to_images(latents):\n",
    "    with torch.no_grad():\n",
    "        latents = latents / vae_scaling_factor\n",
    "        decoded_images = vae.decode(latents.detach()).sample\n",
    "        images = torch.permute(decoded_images, (0, 2, 3, 1))\n",
    "        images = images.to('cpu')\n",
    "        images = numpy.asarray(images)\n",
    "        images = images*0.5+0.5\n",
    "        images = images*255\n",
    "        images = numpy.asarray(images, numpy.uint8)\n",
    "        pass\n",
    "    return images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_relevant_frames_all_images(num_images, latent_size = 64, device = torch.device('cuda')):\n",
    "    relevant_frames = []\n",
    "    #Create noisy latents\n",
    "    latents = torch.randn(num_images, 4, latent_size, latent_size).to(device=device)\n",
    "    #Denoise the latents\n",
    "    for step, t in enumerate(noise_scheduler.timesteps):\n",
    "        with torch.no_grad():\n",
    "            #Get noise prediction\n",
    "            noise_preds = model(latents, t).sample\n",
    "            pass\n",
    "        #Using noise pred and the image at the timestep t, calculate the previous sample\n",
    "        latents = noise_scheduler.step(noise_preds, t, latents).prev_sample\n",
    "        if (step+1) < 800:\n",
    "            if (step+1)%20 == 0:\n",
    "                #Decode the latents\n",
    "                decoded_images = decode_latents_to_images(latents)\n",
    "                relevant_frames.append(decoded_images)\n",
    "                pass\n",
    "            pass\n",
    "        if (step+1) >=800:\n",
    "            if (step+1)%5 == 0:\n",
    "                #Decode the latents\n",
    "                decoded_images = decode_latents_to_images(latents)\n",
    "                relevant_frames.append(decoded_images)\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    return relevant_frames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_relevant_frames_of_one_image(frames, index):\n",
    "    relevant_frames = []\n",
    "    for step, image in enumerate(frames):\n",
    "        image = image[index]\n",
    "        relevant_frames.append(image)\n",
    "        pass\n",
    "    return relevant_frames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def save_gifs(frames, num_images):\n",
    "    for i in range(num_images):\n",
    "        relevant_frames = get_relevant_frames_of_one_image(frames, i)\n",
    "        #print(len(relevant_frames))\n",
    "        # Create the GIF using imageio\n",
    "        with imageio.get_writer(f'DenoisingGiFs/diffusionGif{i}.gif', mode='I') as writer:\n",
    "            for image in relevant_frames:\n",
    "                writer.append_data(image)\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "relevant_frames = get_relevant_frames_all_images(num_images=num_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "save_gifs(frames=relevant_frames, num_images=num_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![GiF](DenoisingGiFs/diffusionGif0.gif)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![GiF](DenoisingGiFs/diffusionGif1.gif)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![GiF](DenoisingGiFs/diffusionGif2.gif)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![GiF](DenoisingGiFs/diffusionGif3.gif)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
